Collections:
  - Name: MixMIM
    Metadata:
      Training Data: ImageNet-1k
      Training Techniques:
        - AdamW
      Training Resources: 16x A100-80G GPUs
      Architecture:
        - MixMIM ViT
    Paper:
        URL: https://arxiv.org/abs/2205.13137
        Title: "MixMIM: Mixed and Masked Image Modeling for Efficient Visual Representation Learning"
    README: configs/selfsup/mixmim/README.md

Models:
  - Name: mixmim-base-p16_16xb128-coslr-300e_in1k
    In Collection: MixMIM
    Metadata:
      Epochs: 300
      Batch Size: 2048
    Results: null
    Config: configs/selfsup/mixmim/mixmim-base-p16_16xb128-coslr-300e_in1k.py
    Weights: https://download.openmmlab.com/mmselfsup/1.x/mixmim/mixmim-base-p16_16xb128-coslr-300e_in1k/mixmim-base-p16_16xb128-coslr-300e_in1k_20221208-44fe8d2c.pth
    Downstream:
      - Type: Image Classification
        Metadata:
          Epochs: 100
          Batch Size: 1024
        Results:
          - Task: Fine-tuning
            Dataset: ImageNet-1k
            Metrics:
              Top 1 Accuracy: 84.63
        Config: configs/selfsup/mixmim/classification/mixmim-base-p16_ft-8xb128-coslr-100e-in1k.py
        Weights: https://download.openmmlab.com/mmselfsup/1.x/mixmim/mixmim-base-p16_16xb128-coslr-300e_in1k/mixmim-base-p16_ft-8xb128-coslr-100e_in1k/mixmim-base-p16_ft-8xb128-coslr-100e_in1k_20221208-41ecada9.pth
